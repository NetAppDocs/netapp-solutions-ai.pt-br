---
sidebar: sidebar 
permalink: infra/ai-lenovo-edge-procedure.html 
keywords: procedure, operating system, ubuntu, nvidia, docker, criteo, brats 
summary: Esta seção descreve os procedimentos de teste usados para validar esta solução. 
---
= Procedimento de teste
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
Esta seção descreve os procedimentos de teste usados para validar esta solução.



== Configuração do sistema operacional e inferência de IA

Para o AFF C190, usamos o Ubuntu 18.04 com drivers NVIDIA e docker com suporte para GPUs NVIDIA e usamos o MLPerf https://github.com/mlperf/inference_results_v0.7/tree/master/closed/Lenovo["código"^] disponível como parte do envio da Lenovo para o MLPerf Inference v0.7.

Para o EF280, usamos o Ubuntu 20.04 com drivers NVIDIA e docker com suporte para GPUs NVIDIA e MLPerf https://github.com/mlcommons/inference_results_v1.1/tree/main/closed/Lenovo["código"^] disponível como parte do envio da Lenovo para o MLPerf Inference v1.1.

Para configurar a inferência de IA, siga estas etapas:

. Baixe os conjuntos de dados que exigem registro, o conjunto de validação ImageNet 2012, o conjunto de dados Criteo Terabyte e o conjunto de treinamento BraTS 2019 e, em seguida, descompacte os arquivos.
. Crie um diretório de trabalho com pelo menos 1 TB e defina a variável de ambiente `MLPERF_SCRATCH_PATH` referindo-se ao diretório.
+
Você deve compartilhar esse diretório no armazenamento compartilhado para o caso de uso de armazenamento de rede ou no disco local ao testar com dados locais.

. Execute a criação `prebuild` comando, que cria e inicia o contêiner docker para as tarefas de inferência necessárias.
+

NOTE: Os comandos a seguir são todos executados de dentro do contêiner docker em execução:

+
** Baixe modelos de IA pré-treinados para tarefas de inferência MLPerf: `make download_model`
** Baixe conjuntos de dados adicionais que podem ser baixados gratuitamente: `make download_data`
** Pré-processar os dados: fazer `preprocess_data`
** Correr: `make build` .
** Crie mecanismos de inferência otimizados para GPU em servidores de computação: `make generate_engines`
** Para executar cargas de trabalho de inferência, execute o seguinte (um comando):




....
make run_harness RUN_ARGS="--benchmarks=<BENCHMARKS> --scenarios=<SCENARIOS>"
....


== Execuções de inferência de IA

Três tipos de execuções foram executadas:

* Inferência de IA de servidor único usando armazenamento local
* Inferência de IA de servidor único usando armazenamento de rede
* Inferência de IA multiservidor usando armazenamento de rede

